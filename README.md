# SiTo: Training-Free and Hardware-Friendly Acceleration for Diffusion Models via Similarity-based Token Pruning (AAAI-2025)
ðŸ“° This is the official code for our paper: [ã€ŠTraining-Free and Hardware-Friendly Acceleration for Diffusion Models via Similarity-based Token Pruningã€‹](https://www.researchgate.net/publication/387204421_Training-Free_and_Hardware-Friendly_Acceleration_for_Diffusion_Models_via_Similarity-based_Token_Pruning)
## ðŸ”¥ News
- `2024/12/10`ðŸ¤—ðŸ¤— SiTo is accepted by AAAI-2025
- `2025/1/18` ðŸ’¥ðŸ’¥ We release the code for our work [SiTo](https://github.com/EvelynZhang-epiclab/SiTo) about accelerating diffusion models for FREE. ðŸŽ‰ **The zero-shot evaluation shows SiTo leads to 1.90x and 1.75x acceleration on COCO30K and ImageNet with 1.33 and 1.15 FID reduction at the same time. Besides, SiTo has no training requirements and does not require any calibration data, making it plug-and-play in real-world applications.**
